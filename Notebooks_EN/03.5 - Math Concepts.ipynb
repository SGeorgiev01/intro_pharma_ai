{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f550749c",
   "metadata": {},
   "source": [
    "# Math Concepts for Machine Learning\n",
    "\n",
    "---\n",
    "### In this lesson you'll learn:\n",
    "\n",
    "- about vectors and matrices and how to do simple calculations with them in Python.\n",
    "- how to calculate the derivative of simple functions.\n",
    "- the difference between a regression and a classification.\n",
    "- how a linear regression functions and the meaning of its coefficients.\n",
    "- about the *Mean Squared Error* and the loss function.\n",
    "- what a logistic regression is and how it relates to linear regressions.\n",
    "- what the Binary Cross Entropy Loss is.\n",
    "- how linear algebra is applied in neural networks.\n",
    "- how the chain rule works.\n",
    "- why the chain rule is so useful for neural networks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c517a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbcc408",
   "metadata": {},
   "source": [
    "# Linear Algebra\n",
    "\n",
    "Today we will explain the essential mathematical principles for neural networks.\n",
    "\n",
    "## Vectors\n",
    "\n",
    "The first essential mathematical concept is the **vector**.\n",
    "\n",
    "A vector represents a point in a space that is described by several values.\n",
    "For example, a molecule can be described by several descriptors. \n",
    "\n",
    "A vector is represented as follows:\n",
    "\n",
    "$$\\begin{bmatrix}3 & 4 & 0.5\\end{bmatrix}$$\n",
    "\n",
    "This vector contains exactly three values. We can use vectors to describe individual data points. For example, we could store the data of a house in this vector. The first value indicates how many bathrooms the house has, the second how many bedrooms, and the third value indicates the age of the heating system in years. This means that every individual data point can be represented as a vector in multidimensional space where each variable (column) represents its own dimension.\n",
    "\n",
    "<table style=\"margin:auto\">\n",
    "\n",
    " <tr>\n",
    "    <th style=\"text-align:center\">Bathrooms</th>\n",
    "    <th style=\"text-align:center\">Bedrooms</th>\n",
    "    <th style=\"text-align:center\">Age of the heating system </th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:center\">3</td>\n",
    "    <td style=\"text-align:center\">4</td>\n",
    "    <td style=\"text-align:center\">0.5</td>\n",
    "  </tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"Img/lin_alg/vector.png\" width=\"600\" style=\"display:block; margin:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b766160",
   "metadata": {},
   "source": [
    "You may have noticed that a vector has amazing similarities to a 1-dimensional `array`.\n",
    "`np.array([3,4,0.5])`. In fact, `np.arrays` are said to have the same functions as vectors. The mathematical rules that apply to vectors also apply to `arrays`.\n",
    "\n",
    "\n",
    "For example, we can multiply a vector by a number: <br>\n",
    "\n",
    "\n",
    "$$3\\cdot\\begin{bmatrix}3 \\\\ 4 \\\\ 0.5\\end{bmatrix}= \\begin{bmatrix}3\\cdot 3 \\\\ 3 \\cdot 4  \\\\ 3 \\cdot 0.5 \\end{bmatrix}= \\begin{bmatrix}9 \\\\ 12 \\\\ 1.5\\end{bmatrix} $$\n",
    "\n",
    "<h5 style=\"text-align:center\"><i>For better overview we write the vector as a column.</i></h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30813ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "3 * np.array([3, 4, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2761c44",
   "metadata": {},
   "source": [
    "The same applies to addition and subtraction:\n",
    "$$3+\\begin{bmatrix}3 \\\\ 4 \\\\ 0.5\\end{bmatrix}= \\begin{bmatrix}3+3 \\\\ 3+4 \\\\ 3+0.5\\end{bmatrix}= \\begin{bmatrix}6 \\\\ 7 \\\\ 3.5\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195a856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "3 + np.array([3, 4, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6509329",
   "metadata": {},
   "source": [
    "We can add two vectors:\n",
    "    \n",
    "    \n",
    "$$\\begin{bmatrix}3 \\\\ 4 \\\\ 0.5\\end{bmatrix} + \\begin{bmatrix}0.3 \\\\ 3 \\\\ -0.2\\end{bmatrix} = \\begin{bmatrix}3 +0.3 \\\\ 4+3 \\\\ 0.5-0.2\\end{bmatrix} =  \\begin{bmatrix}3.3 \\\\ 7 \\\\ 0.3\\end{bmatrix}$$\n",
    "\n",
    "It is important that both vectors have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deccade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([3, 4, 0.5]) + np.array([0.3, 3, -0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8444708c",
   "metadata": {},
   "source": [
    "Vectors become really interesting when we multiply several together.\n",
    "\n",
    "Especially the so-called scalar product (also called a dot product) is important for us and is calculated as follows:\n",
    "$$\\begin{bmatrix}3 \\\\ 4 \\\\ 0.5\\end{bmatrix} \\cdot \\begin{bmatrix}0.3 \\\\ 3 \\\\ -0.2\\end{bmatrix} = (3\\cdot 0.3) + (4 \\cdot 3 )+ (0.5\\cdot -0.2) = 12.8  $$\n",
    "\n",
    "\n",
    "Calculate the scalar product of the vectors by hand: \n",
    "\n",
    "$$\\begin{bmatrix}8 \\\\ 0.25 \\\\ -1\\end{bmatrix} \\cdot \\begin{bmatrix}0.1 \\\\ 12 \\\\ 8\\end{bmatrix} = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50540e2",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution:</strong></summary>\n",
    "\n",
    "$$\\begin{bmatrix}8 \\\\ 0.25 \\\\ -1\\end{bmatrix} \\cdot \\begin{bmatrix}0.1 \\\\ 12 \\\\ 8\\end{bmatrix} =(8\\cdot 0.1) + (0.25 \\cdot 12)+ (-1\\cdot 8) = -4.2  $$\n",
    "</details>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0454e1",
   "metadata": {},
   "source": [
    "In `numpy` we use `np.dot()` to calculate the scalar product. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4606d5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(np.array([3, 4, 0.5]), np.array([0.3, 3, -0.2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3ff8c9",
   "metadata": {},
   "source": [
    "## Matrices\n",
    "\n",
    "You probably already know the linear equation $ y = mx + t $ (or $ y = ax + b $).\n",
    "\n",
    "Have a look at this example about the relationship between the height (German: Größe) and weight (German: Gewicht) of people. The relationship between the two variables $x$ and $y$ can be described via a linear regression.\n",
    "\n",
    "<img src='Img/intro_stats/reg_3.png' alt=\"Drawing\" width=\"500\" style=\"display:block; margin:auto\">\n",
    "\n",
    "$$y = mx + t$$\n",
    "\n",
    "- $x$ is the input variable, in our case the body height\n",
    "- $y$ is the variable to be predicted (body weight)\n",
    "- $m$ describes the slope of the straight line\n",
    "- $t$ denotes the y-axis intercept, the value of $y$ at $x=0$\n",
    "\n",
    "Write a function that calculates the weight using the straight line equation described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee1391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg(x, m, t):\n",
    "    _________  # What is this function supposed to return?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503ca609",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution:</b></summary>\n",
    "    \n",
    "```python\n",
    "def reg(x,m,t):\n",
    "    return m*x+t\n",
    "```\n",
    "</details>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef21aeb",
   "metadata": {},
   "source": [
    "The variable `x` contains the height in cm of 5 people. Calculate the weight for these five people using the function `reg`. Assume that the equation of the regression line is $ \\hat{y} = 0.3x + 21 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b6be66",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [182, 167, 198, 132, 178]\n",
    "y_hat = [reg(__, __, __) for ___ in _____ ]\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a2af29",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution:</b></summary>\n",
    "    \n",
    "```python\n",
    "y_hat = [reg(height,0.3,21) for height in x ]\n",
    "```\n",
    "</details>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388a1cdc",
   "metadata": {},
   "source": [
    "However, we can expand this more common formula to include multiple inputs: $ x_1, x_2, x_3, \\dots $ (a so-called **Multiple Regression**). That would require multiple coefficients which we can call $ \\beta_1, \\beta_2, \\beta_3, \\dots $ and we can substitute the y-intercept $t$ for $\\beta_0$. We then call the prediction value $\\hat{y}$ (`y_hat`) in order to denote that it represents a value calculated from a model and to signify its difference from the true value $y$. In general, this notation is more common when talking about a multiple regression.\n",
    "\n",
    "This gives us the more general formula of:\n",
    "\n",
    "$$ \\hat{y} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3 + \\cdots $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3212ae1",
   "metadata": {},
   "source": [
    "As you may have noticed, the scalar product is similar to a linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c676ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x    = np.array([3, 4, 0.5])\n",
    "beta = np.array([0.3, 3, -0.2])\n",
    "np.dot(x, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188d83bf",
   "metadata": {},
   "source": [
    "However, here `x` is the input vector that contains the information for three variables. For example, for a house that has 3 bathrooms and 4 bedrooms. It was equipped with a new heating system half a year ago (`0.5`). The second vector contains the coefficients of the regression. So $\\beta_1, \\beta_2, \\beta_3$. Using the regression, we can then find the value of the house in 100,000 €. \n",
    "\n",
    "In fact, the scalar product leads to a simplification of the formula. Instead of writing the long version as shown above, we can write it as follows:\n",
    "\n",
    "$$\\hat{y} = x\\beta$$\n",
    "\n",
    "Here we have to assume that $x$ and $\\beta$ are vectors. \n",
    "Of course the $\\beta_0$ (the y-intercept) is still missing. As explained above, single values can simply be added to vectors. \n",
    "\n",
    "So the complete formula is:\n",
    "\n",
    "$$\\hat{y} = x\\beta+\\beta_0$$\n",
    "\n",
    "Can you write this formula using `numpy`? Calculate $\\hat{y}$ for `x`. Use $\\beta_0=-5$ and $ x $ and $ \\beta $ from the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7802b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_0 = -5\n",
    "y_hat = _____________________\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f53c87",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution:</strong></summary>\n",
    "\n",
    "```python\n",
    "y_hat = np.dot(x,beta)+beta_0\n",
    "    \n",
    "```\n",
    "</details>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cb94be",
   "metadata": {},
   "source": [
    "Assuming we want to determine `y_hat` not only for one house but for several houses at the same time, we can do this with exactly the same formula. \n",
    "\n",
    "`X` now contains not only one vector, but several. As you have already learned, such data structures can be stored as a 2D array. In mathematics, a 2D array is comparable to a matrix. You can imagine a matrix as being multiple vectors that have been glued together.\n",
    "\n",
    "When we talk about matrices, we use capitalized variable names.\n",
    "\n",
    "In the following `X` is given. You can see that `np.dot(X,beta) + beta_0` still gives the correct result. But this time for each of the 4 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec9ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[3, 4, 0.5],\n",
    "              [2, 1, 1.2],\n",
    "              [4, 2, 0.12],\n",
    "              [3, 3, 2]])\n",
    "\n",
    "np.dot(X, beta) + beta_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacd7b97",
   "metadata": {},
   "source": [
    "We can extend this knowledge to generalize even further. We can multiply two matrices together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8875db9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.array([beta,\n",
    "              [6, 0, -2],\n",
    "              [1, 0, 3],\n",
    "              [0, 0, -1],\n",
    "              [1, 2, -1]])\n",
    "b_0 = np.array([beta_0, 3, 2, 0.5, -2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adb4f49",
   "metadata": {},
   "source": [
    "`B` now contains the coefficients for a total of five linear regressions. The first row still contains our `beta` coefficients from the first regression.  Each additional row contains new coefficients/weights for another regression. So by the number of rows we can see how many regressions we are running. \n",
    "Also `b_0` contains five values and is therefore now a vector instead of a scalar. For each regression it contains the y-axis intercept.\n",
    "\n",
    "If we now use these two matrices, the following happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99a15e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(X, B) + b_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da07247",
   "metadata": {},
   "source": [
    "An error message:\n",
    "```shapes (4,3) and (5,3) not aligned: 3 (dim 1) != 5 (dim 0)```\n",
    "\n",
    "In fact, we can conclude from the error message what the problem is. \n",
    "First, we are given the dimensions (number of rows and columns). \n",
    "`X` has `4` rows and `3` columns. `B` has `5` rows and `3` columns. \n",
    "\n",
    "Then follows: `3 (dim 1) != 5 (dim 0)`. So, `3 (dim 1)`, the number of columns (`3 (dim 1)`) of the first matrix are not equal (`!=`) to the number of rows in the second matrix (`5 (dim 0)`).   \n",
    "\n",
    "**The number of columns in the first matrix should be equal to the number of rows in the second column. The dimensions must match.**\n",
    "\n",
    "$$ A_{m \\times p} B_{p \\times n} = C_{m \\times n} $$\n",
    "\n",
    "For example, if we flip the `B` matrix by mirroring it \"across the diagonal\" we get rows as columns and columns as rows. Then the number of columns of the first matrix and rows of the second matrix match.\n",
    "\n",
    "Converting columns to rows and vice versa is called the *transpose* of a matrix.\n",
    "`B.tranpose()` performs this transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc6fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(B, \"\\n\")\n",
    "print(B.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4a0b79",
   "metadata": {},
   "source": [
    "As you can see, the rows become columns. This also changes the dimensions of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9849dd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(B.shape, \"\\n\")\n",
    "print(B.transpose().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38efe70b",
   "metadata": {},
   "source": [
    "With the transposition of the matrix `B` the multiplication of the two matrices should work, because now the number of columns/rows is identical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a128a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(X, B.transpose()) + b_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23c8518",
   "metadata": {},
   "source": [
    "It actually works. For example, look at the first column. These values are indeed the results of the first regression we computed: `np.dot(X, beta)+beta_0`.\n",
    "In fact, each row contains the five regression results for one of the four houses.\n",
    "\n",
    "But how can it be that the regression works even though we have flipped the matrix `B`?\n",
    "\n",
    "This is because of how the matrix multiplication has been defined. The scalar product is not calculated between the corresponding rows. The scalar product is calculated between the rows of the first matrix and the columns of the second matrix (row times column, i.e. dimensions of rows and columns must be equal). \n",
    "\n",
    "<img src=\"https://www.mscroggs.co.uk/img/full/multiply_matrices.gif\" style=\"display:block; margin:auto\">\n",
    "<h5 style=\"text-align:center\">Source: Matthew Scroggs - 2020 | www.mscroggs.co.uk/blog/73 |</h5>\n",
    "\n",
    "\n",
    "This is almost all that is needed for the so-called forward pass in a neural network.\n",
    "\n",
    "---\n",
    "The notation with $\\beta$ comes from traditional statistics. In machine learning, the coefficients are denoted by $w$, which stands for \"weights\". In addition, $\\beta_0$, the y-axis intercept, is denoted by $b$ (bias).\n",
    "Thus, the regression equation is:\n",
    "\n",
    "$$Xw+b$$\n",
    "\n",
    "We will keep this spelling for our Machine Learning tasks.\n",
    "\n",
    "As you have already learned, the power of neural networks is that they perform more than one regression at a time.\n",
    "That is, we have not just one set of regression coefficients, but several. How many?\n",
    "That is up to you.\n",
    "\n",
    "In the context of neural networks, the number of regressions performed corresponds to the number of nodes in the hidden layer of the neural network.\n",
    "\n",
    "Until now we have always used `np.dot()` for a matrix multiplication. But there is an extra function `np.matmul()`. For large matrices `np.matmul` is faster and therefore we will also use this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402231cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(X, B.transpose()) + b_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a494c756",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7dcad7",
   "metadata": {},
   "source": [
    "Let's circle back to the simple linear regression we had at the start. We need a way to not only create but also to evaluate the predictive power of linear regressions. Therefore, we need some way to calculate the difference between the true value for $ y $ and the predicted value $ \\hat{y} $. Then we can compare models between each other.\n",
    "\n",
    "This difference between the actual and the predicted value ($y - \\hat{y}$) is also called the residual. The symbol for the residual is usually the small epsilon ($\\epsilon$), which is used to measure the magnitude of the error (**E**rror) of the prediction. \n",
    "\n",
    "<img src='Img/intro_stats/reg_4.png' alt=\"Drawing\" width=\"500\" style=\"display:block; margin:auto\">\n",
    "\n",
    "For example, to estimate how good a model is overall, we could simply sum the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ccdf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [182, 167, 198, 132, 178]\n",
    "y_hat = [reg(height,0.3,21) for height in x ]\n",
    "\n",
    "y = np.array([78.2, 68.3, 81.0, 64.3, 70.1])\n",
    "y_hat = np.array(y_hat)\n",
    "residual = y - ___  # What do we substract from y?\n",
    "\n",
    "sum(residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432c8804",
   "metadata": {},
   "source": [
    "As you can see, the value is very close to zero, a very small error. The problem, however, is that the residuals can be both positive and negative. That is, when you add them together, they cancel each other out. You will always get values close to zero. To avoid this, we do not sum the residuals, but we sum the squares of the residuals. $$\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2$$ \n",
    "\n",
    "However, the sum alone would lead to models with more data points, i.e., with a larger $n$, automatically having larger error sums. Therefore, we take the mean of the squares instead of the sum: $\\frac{1}{n}\\sum_{i=1}^{n}(y_-\\hat{y}_i)^2$. This value, called the *Mean Squared Error* (MSE), is useful to assess the quality of the predictions. If a model has a small MSE, one can conclude that the residuals must be small, i.e., the differences between the predicted and true values are small. \n",
    "\n",
    "As with the variance and standard deviation, there is also the root mean squared error (RMSE). As you can guess, this is simply taken by taking the square root of the MSE. Write a function that can calculate the RMSE. You can use `numpy`, i.e. you do not need a `for-loop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36e554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y, y_hat):\n",
    "   MSE = np.sum(__________________) / len(_____)  # calculate the MSE here\n",
    "   return ___________  # convert the MSE to the RMSE\n",
    "\n",
    "\n",
    "RMSE(y, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ebae5c",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution:</b></summary>\n",
    "    \n",
    "```python\n",
    "def RMSE(y,y_hat):\n",
    "   MSE = np.sum((y-y_hat)**2)/len(y)\n",
    "   return np.sqrt(MSE) \n",
    "```\n",
    "</details>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bc35ab",
   "metadata": {},
   "source": [
    "In machine learning or in the field of optimization in general, functions like the RMSE are also called loss functions. They measure how well a model fits the data. The loss calculated by these functions must be minimized. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff4a73e",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Up to now you have always been given the parameters `m` and `t`. In reality, you have to calculate them yourself. In the following example we deal with the prediction of the boiling point. For this we use a data set from the American *National Institute of Standards and Technology*. In the data set, the boiling temperatures for 72 simple alcohols are recorded. In addition, the molecular weight and the number of carbons are given. \n",
    "The data set is located in the folder `../data/boilingpoints/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2995f265",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://uni-muenster.sciebo.de/s/qGVs59xsnWKKuIf/download\").values\n",
    "print(\"Dimensions of the data: \", data.shape)\n",
    "data[:10, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908f11d9",
   "metadata": {},
   "source": [
    "The data set consists of 72 rows and three columns. Each row represents an alcohol and the three columns contain information for one of the three descriptors. The first column contains the boiling points, the second the molecular weight and the third column the number of carbons. \n",
    "\n",
    "Our goal is to predict the boiling point based on the molecular weight.\n",
    "First, we store the first column (boiling points) in the variable `y` and the second column in the variable `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a1052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[:, 0]  # y the variable we want to predict (boiling points)\n",
    "x = data[:, 1:2]  # we could also use data[:,1], but behaves differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[:5, 1])\n",
    "print(data[:5, 1:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94427780",
   "metadata": {},
   "source": [
    "You can see that we select the same values in the example, but in the first variant we reduce the column to a 1-dimensional array of size `(72)`. So a vector of length 72. Some of the functions necessary for linear regression expect our variable `x` to be in the form of a 2-dimensional array. Therefore we select the column with `data[:,1:2]`. Thus we keep the 2D structure of the `array`.\n",
    "\n",
    "We can also plot the data using the library `matplotlib`. With the function `plt.plot()` you can quickly create simple plots. Here you just have to specify what values belong on the x-axis (first position in the function), then specify what belongs on the y-axis (second position). Finally, you can specify whether the individual values should be plotted as a point `\"o\"` or connected with a line `\"-\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfb65d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x, y, \"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2405e9",
   "metadata": {},
   "source": [
    "It can be clearly seen that as the weight increases, the boiling point of the alcohols also increases. \n",
    "\n",
    "In the next cell, we calculate the linear regression parameters that fit the data. \n",
    "For this we need the Python library `sklearn`, which provides many functions for statistical analysis and machine learning.\n",
    "\n",
    "Regardless of which `sklearn` model you want to use, the general structure remains the same. \n",
    "First, the type of the model must be defined.\n",
    "Using `model = LinearRegression()` tells Python to create a linear regression model.\n",
    "\n",
    "Next, the model must be *fitted* to the data `(x,y)`. This is done with the `model.fit(x,y)` statement. This step leads to the calculation of the regression parameters.\n",
    "\n",
    "We get the estimated parameters via `model.coef_[0]` for the slope (`m` or `beta`) and `model.intercept_` for the y-axis intercept (`t` or `beta_0`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)  # calculates the linear regression\n",
    "m = model.coef_[0]  # we can get m and t from the model\n",
    "t = model.intercept_\n",
    "\n",
    "print(m, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4727e372",
   "metadata": {},
   "source": [
    "Calculate `y_hat` with the parameters and then the RMSE. Since we are now using `np.arrays`, no `for-loop` is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa01b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = reg(data[:, 1], ___, ____)\n",
    "RMSE(y, ____)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39215658",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution:</b></summary>\n",
    "    \n",
    "```python\n",
    "y_hat = reg(data[:,1], m , t)\n",
    "RMSE(y, y_hat) \n",
    "```\n",
    "</details>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8043f85a",
   "metadata": {},
   "source": [
    "Can you find other values for \"m\" and \"t\" that result in a lower RMSE?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0c8120",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = reg(data[:, 1], ____,  _____)\n",
    "RMSE(y, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9acffcc",
   "metadata": {},
   "source": [
    "In fact, this does not work. When we speak of a linear regression, we usually mean an *ordinary least-square* regression. As the name implies, this regression minimizes squares, the error of the regression line. That is, the regression line is the optimal line that can be found for that data set. In other words, an OLS regression line minimizes the (R)MSE.\n",
    "\n",
    "## Multiple Regression\n",
    "\n",
    "As we've already seen, linear regression can also be performed with more than one $x$ variable. The formula expands to:\n",
    "\n",
    "$$\\hat{y}= \\beta_0 +\\beta_1x_1 +\\beta_2x_2$$\n",
    "\n",
    "The interpretation of these coefficients does not change.\n",
    "\n",
    "We can use both the number of carbons and the weight to predict the boiling points.\n",
    "\n",
    "For this to work, you must first select not only the second but also the third column of `data` in `x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[:, 1: ___ ]  # Which columns do we need for x?\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be29ec4c",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution:</b></summary>\n",
    "    \n",
    "```python\n",
    "x = data[:,1:3]\n",
    "```\n",
    "</details>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5cc036",
   "metadata": {},
   "source": [
    "You can now have the regression coefficients estimated again with `LinearRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af96d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = LinearRegression()\n",
    "model_2.fit(x, y)  # calculates the linear regression\n",
    "print(model_2.coef_, model_2.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc155d0",
   "metadata": {},
   "source": [
    "As you can see, you now get a total of 3 parameters. The regression coefficient for the molecular weight is `-4.65` and for the number of carbon `83.18`. `sklearn` also has a function `predict()`. With it we can automatically make predictions with the previously estimated parameters. In the following example, we used this function to calculate `y_hat` for the `x` values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model_2.predict(x)\n",
    "RMSE(y, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d446a8a",
   "metadata": {},
   "source": [
    "By using another variable in the regression, we were able to almost halve the loss (RMSE). This means that the model with two input variables leads to significantly better predictions than the first model with only one input variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b563b04",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "There are also problems where exact values are not to be predicted. For example, we want to decide whether a patient needs to be admitted to the intensive care unit or not. Here we only have to decide between `YES` and `NO`. Mathematically, however, we would speak of `1` or `0`. When a data point can belong to one of two groups, we speak of a **binary classification**. \n",
    "\n",
    "Here we have an example of a basketball player who throws at the hoop from different distances. \n",
    "If he scores, this throw is rated as a `1`. If he does not, the throw is rated with a `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d311663",
   "metadata": {},
   "outputs": [],
   "source": [
    "throws = np.array([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
    "distance = np.array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14.,\n",
    "                     15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27., 28., 29.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca231812",
   "metadata": {},
   "source": [
    "It is possible to calculate a simple regression line, but it does not fit the data very well because of the binary variable $y$. One solution is logistic regression. Here, a sigmoid function \"after\" linear regression is used to transform the predicted values. \n",
    "\n",
    "<table style=\"margin:auto\"><tr>\n",
    "<td> <img src='Img/intro_stats/log1.png' alt=\"Drawing\" style=\"width: 250px;\"> </td>\n",
    "<td> <img src='Img/intro_stats/log2.png' alt=\"Drawing\" style=\"width: 250px;\"> </td>\n",
    "<td> <img src='Img/intro_stats/log3.png' alt=\"Drawing\" style=\"width: 250px;\"> </td>\n",
    "</tr></table>\n",
    "<br>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<h2 style=\"text-align:center\">Sigmoid Function</h2>\n",
    "\n",
    "\n",
    "The sigmoid function is a non-linear function. Mathematically, the sigmoid function is written like this:\n",
    "$$sigmoid(z)= \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "To understand what it does exactly, you can take a look at the example.\n",
    "\n",
    "<td><img src='Img/intro_stats/sigmoid.png' alt=\"Drawing\" style=\"width: 550px; display:block; margin:auto\">\n",
    "<h5 style=\"text-align:center\">x-axis: before applying the sigmoid function<br>y-axis: after applying the sigmoid function</h5>\n",
    "\n",
    "On the x-axis are values between -6 and 6, **before** the sigmoid function is applied to these values. On the y-axis are the same values, but this time after applying the sigmoid function. \n",
    "All values are now between 0 and 1. Values that were very far from 0 before are now very close to `0` or `1`.\n",
    "    \n",
    "The shape of this function fits much better to a binary classification.\n",
    "\n",
    "To perform a logistic regression, we can build on what we have already learned.\n",
    "We have the same situation, we want to make a prediction for `y` based on our inputs `x`.     \n",
    "\n",
    "To do this, we simply substitute the values from the linear regression into the sigmoid function.\n",
    "$$ z = \\beta x + \\beta_0 $$\n",
    "$$\\hat{y} = sigmoid(z) = \\frac{1}{1+e^{-z}} = \\frac{1}{1+e^{-(\\beta x + \\beta_0)}} $$\n",
    "\n",
    "Now calculate `z` by applying `reg` to the `distance` values. Since you can now use `numpy`, you no longer need a `for-loop`.\n",
    "For the example with the basketball player, the following parameters are given:\n",
    "- `m` = -0.8\n",
    "- `t` = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = reg(____)  # it is no longer convention to call our input variable x\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ebb50d",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution:</b></summary>\n",
    "\n",
    "```python\n",
    "z = reg(distance,-0.8,7) \n",
    "```\n",
    "</details>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9400dc",
   "metadata": {},
   "source": [
    "Next you need the sigmoid function. For this write a function in Python with `numpy`. $e^x$ can be written as `np.exp(x)` with `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(value):\n",
    "    return 1 / (___________)  # wrie the denominator of the sigmoid function here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23938083",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution:</b></summary>\n",
    "    \n",
    "```python\n",
    "def sigmoid(value):\n",
    "    return 1/(1+np.exp(-value))\n",
    "```\n",
    "</details>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56abeaa6",
   "metadata": {},
   "source": [
    "In the last step, calculate `y_hat` using `z` and the `sigmoid` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49cdcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = sigmoid(_____)  # Which input do you need for the sigmoid function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aed9f50",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution:</b></summary>\n",
    "    \n",
    "```python\n",
    "y_hat = sigmoid(z)\n",
    "```\n",
    "</details>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86867934",
   "metadata": {},
   "source": [
    "As you can see, all values are now between `0` and `1`. Actually, we wanted values that are exacly `0` or `1`, not values in between. But the values of `y_hat` can be understood as a kind of probability. A predicted value of `0.99908895` means that, according to the model, the basketball player will score a basket 0.99% of the time. Conversely, a value of `0.00135852 means that, according to the model, there is only a 0.14% chance of scoring a basket.\n",
    "\n",
    "The following figure shows the predicted values together with the predicted images. \n",
    "\n",
    "<img src='Img/intro_stats/log4.png' alt=\"Zeichnung\" width=\"500px\" style=\"display:block; margin:auto\">\n",
    "\n",
    "Normally, the probabilities are interpreted in such a way that the model predicts a `1`, i.e. a hit, from a value `>= 0.5` and a `0` (miss) below.\n",
    "\n",
    "Thus, we can judge the accuracy of the model by the percentage of correctly classified throws. \n",
    "First, we round `y_hat`. This gives us only `0` and `1` as predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2f0975",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.round(y_hat)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d151ef2",
   "metadata": {},
   "source": [
    "You can now compare whether `pred` matches the original `y` variable `throws`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred == throws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6605ad2f",
   "metadata": {},
   "source": [
    "Write a function to calculate the accuracy (percentage of correctly classified throws). Remember that `booleans`, i.e., `True` and `False`, can also be written as `1` or `0` in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cffb3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    return np.sum(y_true == ___) / len(____)\n",
    "\n",
    "accuracy(throws, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd2294a",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution:</b></summary>\n",
    "    \n",
    "```python\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.sum(y_true==y_pred)/len(y_true)\n",
    "```\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965f933d",
   "metadata": {},
   "source": [
    "## Binary Cross Entropy Loss\n",
    "\n",
    "An accuracy of 0.73 means that the model predicts the correct result 73% of the time. Similar to the RMSE, this is a metric to estimate how good our model is.\n",
    "\n",
    "Often, however, not just one metric is used. The advantage of accuarcy is that it is very easy to interpret. But some mathematical properties of accuarcy make it unsuitable for certain machine learning methods. Therefore, at least two different metrics are usually used. \n",
    "\n",
    "The additional metric used in classification is the **Cross Entropy** Loss. In the case of a binary classification problem, it is usually referred to as **Binary Cross Entropy** (BCE) Loss. \n",
    "\n",
    "$$Loss =-\\frac{1}{n}\\sum_{i=0}^n[y_i\\cdot log(\\hat{y}_i) + (1-y_i)\\cdot log(1-\\hat{y}_i)]$$\n",
    "\n",
    "The formula looks very complicated at a first glance, but it is relatively easy to understand with the help of examples.\n",
    "Let's assume we want to calculate the loss for only one data point, e.g. for a single shot of the basketball player. Then $n = 1$ and the above formula simplifies:\n",
    "\n",
    "\n",
    "$$Loss =-[y_i\\cdot log(\\hat{y}_i) + (1-y_i)\\cdot log(1-\\hat{y}_i)]$$\n",
    "\n",
    "\n",
    "##### Assuming that the basketball player did not hit the shot, then $y_i=0$.\n",
    "\n",
    "<img src='Img/intro_stats/bce_1.gif' alt=\"Drawing\" width= \"500px\" style=\"display:block; margin:auto\">\n",
    "\n",
    "Resulting in:\n",
    "\n",
    "$$\\begin{align}\n",
    "Loss&=-0\\cdot log(\\hat{y}_i) + (1-0)\\cdot log(1-\\hat{y}_i)\\\\\n",
    "&=-log(1-\\hat{y}_i)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "That is, the loss for this shot is the $log$ of the difference of 1 and $\\hat{y}$ (the predicted probability).\n",
    "\n",
    "You can try out what happens to the loss for different probabilities. Remember that the true value is $y_i=0$. So a good model would predict a low probability, so a small loss is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909799dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put different probabilities into the formula below and see what happens to the loss\n",
    "\n",
    "np.log(1 - 0.___)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6084bbde",
   "metadata": {},
   "source": [
    "First of all, you will notice that the loss is always negative, which is why there is a minus in the actual formula from above to make the loss positive again. \n",
    "\n",
    "You can see that for particularly high probabilities, the loss moves away from zero. For particularly small probabilities, the loss approaches zero. This means that the more \"wrong\" our model is, the greater the loss, and that is exactly what we want.\n",
    "\n",
    "##### Assuming that our basketball player has hit the shot, then $y_i=1$\n",
    "<img src='Img/intro_stats/bce_2.gif' alt=\"Drawing\" width= \"500px\" style=\"display:block; margin:auto\">\n",
    "\n",
    "$$\\begin{align}Loss &=-1\\cdot log(\\hat{y}_i) + (1-1)\\cdot log(1-\\hat{y}_i)\\\\\n",
    "Loss &=-log(\\hat{y}_i)\\end{align}$$\n",
    "\n",
    "This time, a different but still simple part of the formula remains.\n",
    "Try this term with different probabilities as well. \n",
    "This time a probability close to 1 would be correct, which should result in a small loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-np.log(0.___)  # try different probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ce4cdb",
   "metadata": {},
   "source": [
    "Again, the loss increases as the probability moves away from the true value. \n",
    "\n",
    "The loss is therefore only complex enough to cover both a true value of `1` and `0`.  The factor $log$ is used so that values further away from the true value have a disproportionate effect on the loss. The previously ignored part of the formula $\\frac{1}{n}\\sum_{i=1}^n$ only calculates the average over all data points in the data set. \n",
    "\n",
    "In the following, the formula for the BCE is defined using `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BCE(y_true, y_hat):\n",
    "    return -np.mean(y_true*np.log(y_hat) + (1 - y_true)*np.log(1 - y_hat))\n",
    "\n",
    "BCE(throws, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609c730f",
   "metadata": {},
   "source": [
    "# Calculus\n",
    "\n",
    "## Derivatives\n",
    "\n",
    "In order to understand how neural networks learn, one should know at least roughly what derivatives are and how to calculate them.\n",
    "\n",
    "The derivative of a function describes the slope of the original function. \n",
    "Suppose there is a function $f(x)=x^2$. Then the corresponding derivative $\\frac{df}{dx}=2x$ (i.e.: *Derivative of f with respect to x*). \n",
    "\n",
    "In the picture $f(x)$ (*blue*) as well as the derivative $\\frac{df}{dx}$ (*orange*) are drawn. <br>For example, for $x=-5$, $f(-5) = 25$. The slope at this point is: $\\frac{df(-5)}{dx}=2\\cdot -5= -10$. That is, the slope of the function $f(x)=x^2$ is $-10$ when $x=-5$.\n",
    "\n",
    "<img src=\"Img/lin_alg/ableitung_1edit.png\" width=\"1000\" style=\"display:block; margin:auto\">\n",
    "\n",
    "There are some rules about derivation. First, a simple rule with an example: \n",
    "        $$f(x) = x^n \\rightarrow \\frac{df}{dx} = n \\cdot x^{n-1}$$\n",
    "        $$f(x) = x^2 \\rightarrow \\frac{df}{dx} = 2 \\cdot x^{2-1}=2x^1= 2x $$\n",
    "        \n",
    "\n",
    "In principle, constants are always dropped in derivatives.\n",
    "\n",
    "That is:\n",
    "The derivative of $f(x)=x^2 + 5$ is only $2x$, since constants only shift the function, but do not affect its slope. \n",
    "\n",
    "Coefficients are handled differently:\n",
    "\n",
    "$$f(x) = ax^n \\rightarrow \\frac{df}{dx} = (n \\cdot a)\\cdot x^{n-1}$$\n",
    "\n",
    "Example:\n",
    "\n",
    "$$f(x) = 4x^3 \\rightarrow \\frac{df}{dx} = 12x^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d48da1",
   "metadata": {},
   "source": [
    "**Try to find the derivative of the following functions (probably easier on paper):**\n",
    "\n",
    "$$g(x)= 7x^5 - 3$$\n",
    "\n",
    "$$h(x)= 0.5x^2 + 3x +12$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5295c4ef",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution:</strong></summary>\n",
    "\n",
    "$$\\frac{dg}{dx} = 35x^4 $$\n",
    "$$\\frac{dh}{dx} = x +3$$\n",
    "\n",
    "</details>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14620a2e",
   "metadata": {},
   "source": [
    "### Chain Rule \n",
    "\n",
    "The most important rule for neural networks is the chain rule, where derivatives of chained functions is calculated, i.e. general functions of the type: $$f(x) = g(h(x))$$ The derivative of such a function is then: $$\\frac{df}{dx} = \\frac{dg}{dh}\\cdot \\frac{dh}{dx}$$ Based on the formula it might be difficult to understand, but using an example it should be relatively easy.\n",
    "\n",
    "$$\\begin{align}f(x)&= (3x + 1)^2 \\\\g(h)&=h^2; \\space\\space\\space\\space\\space\\space h(x) = 3x+1\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{df}{dx} &= \\frac{d}{dh} (h^2)\\cdot \\frac{d}{dx}h\\\\\n",
    "&= 2 h\\cdot \\frac{d}{dx}(3x+1)\\\\\n",
    "&= 2 h \\cdot 3 \\\\\n",
    "&= 6 \\cdot (3x+1)\n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "\n",
    "Previously it was said that the derivative describes the slope of the original function. One can also interpret the derivative $\\frac{df}{dx}$ as follows: *By how much does $f(x)$ change if I change $x$?* Here, of course, the amount of change depends on $x$ itself. In the $x^2$ example, small changes in $x$ have a greater impact for values around $x=5$ than for values around $x=1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baae07c2",
   "metadata": {},
   "source": [
    "### Example:\n",
    "\n",
    "$$e_1 = 2x+3$$\n",
    "$$e_2 = 0.5e_1^3$$\n",
    "\n",
    "Try calculating $\\frac{de_2}{dx}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce4a031",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution:</strong></summary>\n",
    "\n",
    "$$\\frac{de_2}{dx}= \\frac{de_1}{dx}\\frac{de_2}{de_1} $$\n",
    "$$\\frac{de_2}{dx}= 2(1.5e_1^2) $$\n",
    "    \n",
    "Because we know that $e_1 = 2x+3$, we can write.\n",
    "$$\\frac{de_2}{dx}= 2(1.5(2x+3)^2) $$\n",
    "$$\\frac{de_2}{dx}= 2(1.5(4x^2+12x+9)) $$\n",
    "$$\\frac{de_2}{dx}= 2(6x^2+18x+13.5) $$\n",
    "$$\\frac{de_2}{dx}= 12x^2+36x+27 $$\n",
    "</details>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e7d0a5",
   "metadata": {},
   "source": [
    "## Chain Rule\n",
    "\n",
    "Previously it was said that the derivative describes the slope of the original function. One can also interpret the derivative $\\frac{df}{dx}$ as follows: *By how much does $f(x)$ change if I change $x$?* Here, of course, the amount of change depends on $x$ itself. In the $x^2$ example, small changes in $x$ have a greater impact for values around $x=5$ than for values around $x=1$. \n",
    "\n",
    "If we want to optimize the weights of a neural net, we also need to know how a change in the weights causes a change in the loss. \n",
    "\n",
    "Here again is a schematic of a neural network.\n",
    "\n",
    "\n",
    "<img src=\"Img/lin_alg/ableitung_3edit.png\" style=\"display:block; margin:auto\">\n",
    "\n",
    "For the following example we consider only the last part in more detail. The calculation of $\\hat{y}$ is done in two steps. First $Z_2$ is calculated, then a nonlinear function is applied to it, which gives us $\\hat{y}$. \n",
    "\n",
    "<img src=\"Img/lin_alg/ableitung_4edit.png\" style=\"display:block; margin:auto\">\n",
    "\n",
    "**For simplicity, we consider only single values in this example.**\n",
    "\n",
    "So $a_1$ is not a vector at this moment, but only a single value, the same is true for $w_2$ and $b_2$.\n",
    "\n",
    "<img src=\"Img/lin_alg/ableitung_5.png\" style=\"display:block; margin:auto\">\n",
    "\n",
    "\n",
    "The question is: What influence does $w_2$ / $b_2$ have on the loss $J$. Or how does the loss change when we change $w_2$ / $b_2$?\n",
    "\n",
    "Mathematically, we can call this the derivative of $J$ with respect to $w_1$. \n",
    "We now use $\\partial$ instead of $d$ since we are talking about functions with multiple parameters ($w_2$ and $b_2$) - this is a so-called partial derivative. In the example below, we fix the dimension of $b_2$ in place and only look at what effect a small change in $w_2$ has on the loss $J$.\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_2}$$\n",
    "\n",
    "However, there is no direct influence of $w_2$ on the loss. $w_2$ influences $z_2$ and $z_2$ has an effect on $\\hat{y}$. And finally, $\\hat{y}$ has an effect on the loss. So the functions to calculate $\\hat{y}$ and $J$ respectively are *chained*.\n",
    "\n",
    "The chain rule allows us to calculate $\\frac{\\partial J}{\\partial w_2}$ in exactly this way.\n",
    "\n",
    "First, we calculate the effect of $w_2$ on $z_2$:\n",
    "$$\\frac{\\partial J}{\\partial w_2} = \\frac{\\partial z_2}{\\partial w_2}.... $$\n",
    "\n",
    "Next, the effect of $z_2$ on $\\hat{y}$:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_2} = \\frac{\\partial z_2}{\\partial w_2}\\frac{\\partial \\hat{y}}{\\partial z_2} $$\n",
    "\n",
    "Last, the effect of $\\hat{y}$ on $J$:\n",
    "\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_2} = \\frac{\\partial z_2}{\\partial w_2}\\frac{\\partial \\hat{y}}{\\partial z_2}\\frac{\\partial J}{\\partial \\hat{y}} $$\n",
    "\n",
    "\n",
    "The chain rule allows us to simply multiply these effects to get the desired derivative.\n",
    "This chain can become arbitrarily long, therefore a network can also become arbitrarily large. \n",
    "Since, as you may recall, there is also a $w_1$ and $b_1$, their effect on $J$ can also be calculated. For this the chain rule works the same way, the \"chain\" only becomes longer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da058783",
   "metadata": {},
   "source": [
    "## Practice Exercise\n",
    "\n",
    "In this exercise you will also calculate the gradient for $w$ as in a neural network. \n",
    "Simplified, of course, and only for one value of $w$. In this example we use a simple loss function and also not a real non-linear function. The loss function would not work in the real application. The same is true for the nonlinear function, since here it is linear. A non-linear function would be beyond the scope of this exercise.\n",
    "\n",
    "\n",
    "Please try to solve this exercise to the best of your ability. As we have said many times, it is not important for us that you get the correct result, but that you have studied the subject. Some people find math easier than others, we are aware of that. \n",
    "\n",
    "\n",
    "\n",
    "Back to our \"faux\" neural network.\n",
    "Let's assume that the last layer of our network works as follows:\n",
    "\n",
    "$$z_2 = a_1w_2+b_2$$\n",
    "$$\\hat{y} = z_2^3-3$$\n",
    "$$J = \\hat{y}^2- y^2$$\n",
    "\n",
    "\n",
    "Calculate $\\frac{\\partial J}{\\partial w_2}$, the \"influence\" $w_2$ has on $J$ (Loss).\n",
    "For this we give the values:\n",
    "\n",
    "$$ a_1 = 2 $$\n",
    "$$ b_2=1.4 $$\n",
    "$$ w_2 =0.6 $$\n",
    "$$ y=1 $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab32600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate first z_2, y_hat, and J. A simplified forward pass.\n",
    "weight = 0.6\n",
    "\n",
    "z_2 = ___*weight + ___\n",
    "\n",
    "y_hat = (z_2**__) - ___\n",
    "\n",
    "J = ____-____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b821c3",
   "metadata": {},
   "source": [
    "You have performed the forward pass, now follows the calculation of the gradients. To do this, we first need to calculate only the individual derivatives.\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_2} = \\frac{\\partial z_2}{\\partial w_2}\\frac{\\partial \\hat{y}}{\\partial z_2}\\frac{\\partial J}{\\partial \\hat{y}} $$\n",
    "\n",
    "First you calculate $\\frac{\\partial z_2}{\\partial w_2}$ which we will call `dw_2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4eb25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_2 = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fc5ec4",
   "metadata": {},
   "source": [
    "Next, you calculate $\\frac{\\partial \\hat{y}}{\\partial z_2}$, we'll call it `dz_2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439c7a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "dz_2 = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b552c9",
   "metadata": {},
   "source": [
    "Finally, you'll calculate $\\frac{\\partial J}{\\partial \\hat{y}}$, we'll call it `dy_hat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85998896",
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_hat = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5626447",
   "metadata": {},
   "source": [
    "To calculate the gradient, you now only need to multiply these three together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1536658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = dw_2 * dz_2 * dy_hat\n",
    "gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da09229",
   "metadata": {},
   "source": [
    "That's it! You have calculated the gradients.\n",
    "\n",
    "**You don`t have to submit the following task, but you can try your hand at it.**\n",
    "\n",
    "If we put these derivatives in a `for-loop` and change the weighting a bit against the gradients, we can see that the loss slowly gets smaller, we are training the \"neural network\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef134af",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = 0.6\n",
    "for i in range(10):\n",
    "    z_2 = ___*weight + ___\n",
    "    y_hat = (z_2**__) - ___\n",
    "    J = ____-____\n",
    "    dw_2 = \n",
    "    dz_2 = \n",
    "    dy_hat = \n",
    "    gradient = dw_2 * dz_2 * dy_hat\n",
    "    weight -=  0.0001 * gradient  # updating the weights\n",
    "    print(J)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "softuni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
